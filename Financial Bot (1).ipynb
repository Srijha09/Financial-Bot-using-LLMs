{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a8b7de-a8ee-48f5-93f6-63043ca235b7",
   "metadata": {},
   "source": [
    "Developing Financial Bot using LLaMA 2 and RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1186ecb-a2e9-42ae-8738-8a918f447800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.34.51)\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.214.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.51 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.34.51)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.21.12)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.10.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.1.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.2)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.2.0)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.0.0)\n",
      "Collecting urllib3<3.0.0,>=1.26.8 (from sagemaker)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker) (7.0.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.51->boto3) (2.9.0)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.20.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.8)\n",
      "Requirement already satisfied: dill>=0.3.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.16)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.6\n",
      "    Uninstalling urllib3-1.26.6:\n",
      "      Successfully uninstalled urllib3-1.26.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.4 which is incompatible.\n",
      "tensorflow 2.12.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed urllib3-2.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efece651-e5f8-4cad-89dd-7a5b537af2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the financial report corpus ...\n",
      "Instantiating Text Splitter...\n",
      "Preparing Vector Embeddings...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "from __future__ import annotations\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Get Env Variables\n",
    "AWS_REGION='us-east-1'\n",
    "EMBEDDING_MODEL='sentence-transformers/all-MiniLM-L6-v2'\n",
    "LLAMA2_ENDPOINT='jumpstart-dft-meta-textgeneration-l-20240510-040141'\n",
    "INFERENCE_COMPONENT = 'meta-textgeneration-llama-2-7b-f-20240510-040141'\n",
    "MAX_HISTORY_LENGTH=10\n",
    "\n",
    "\n",
    "# Step1: Define a sentence transformer model that will be used\n",
    "#        to convert the documents into vector embeddings\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "loader = PyPDFLoader(r\"data/tsla-20240331-gen.pdf\")\n",
    "\n",
    "# load your data\n",
    "print('Loading the financial report corpus ...')\n",
    "data = loader.load()\n",
    "# Text splitter\n",
    "print('Instantiating Text Splitter...')\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=300)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "    \n",
    "# Step3: Create & save a vector database with the vector embeddings\n",
    "#        of the documents\n",
    "print('Preparing Vector Embeddings...')\n",
    "db = FAISS.from_documents(all_splits, embeddings)\n",
    "db.save_local(\"faiss_index\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9912854c-623d-41c5-88b2-67c288126a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_document_content(document):\n",
    "    # Extract the text within the <document_content> tags\n",
    "    start_tag = \"<document_content>\"\n",
    "    end_tag = \"</document_content>\"\n",
    "    start_index = document.find(start_tag) + len(start_tag)\n",
    "    end_index = document.find(end_tag)\n",
    "    return document[start_index:end_index].strip()\n",
    "    \n",
    "def build_chain():\n",
    "    print('Preparing chain...')\n",
    "    # Sentence transformer\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "    # Laod Faiss index\n",
    "    db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "\n",
    "    # Define the Prompt for the \n",
    "    system_prompt = \"\"\"You are an assistant for question-answering tasks for Retrieval Augmented Generation system for the financial reports such as 10Q and 10K.\n",
    "                        Use the following pieces of retrieved context to answer the question. \n",
    "                        If the answer is directly available in the context, provide the precise answer.\n",
    "                        If the answer is not directly available or cannot be inferred from the context, say that the information is not available to answer the question.\n",
    "                        Use two sentences maximum and keep the answer concise.\n",
    "                        Question: {question} \n",
    "                        Context: {context} \n",
    "                        Answer:\"\"\"\n",
    "\n",
    "\n",
    "    # Custom ContentHandler to handle input and output to the SageMaker Endpoint\n",
    "    class ContentHandler(LLMContentHandler):\n",
    "        content_type = \"application/json\"\n",
    "        accepts = \"application/json\"\n",
    "\n",
    "        def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "            payload = {\n",
    "                \"inputs\": json.dumps([\n",
    "                    [\n",
    "                        {\n",
    "                            \"role\": \"system\", \"content\": system_prompt,\n",
    "                        },\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                ]),\n",
    "                \"parameters\": {\"max_new_tokens\": 1000, \"top_p\": 0.8, \"temperature\": 0.4},\n",
    "            }\n",
    "            input_str = json.dumps(payload)\n",
    "            return input_str.encode(\"utf-8\")\n",
    "\n",
    "        def transform_output(self, output: bytes) -> str:\n",
    "            try:\n",
    "                response_str = output.read().decode(\"utf-8\")\n",
    "                response_json = json.loads(response_str)\n",
    "                #print(f\"Response JSON: {response_json}\")  # Debug print\n",
    "\n",
    "                if isinstance(response_json, list) and len(response_json) > 0 and 'generated_text' in response_json[0]:\n",
    "                    content = response_json[0]['generated_text']\n",
    "                    return content.split(\"Answer:\")[-1].strip()\n",
    "                else:\n",
    "                    return \"The information is not available to answer the question.\"\n",
    "            except (json.JSONDecodeError, KeyError, IndexError, ValueError) as e:\n",
    "                print(f\"Error parsing response: {e}\")\n",
    "                return \"The information is not available to answer the question.\"\n",
    "\n",
    "\n",
    "    # Langchain chain for invoking SageMaker Endpoint\n",
    "    llm = SagemakerEndpoint(\n",
    "        endpoint_name=LLAMA2_ENDPOINT,\n",
    "        region_name=AWS_REGION,\n",
    "        content_handler=ContentHandler(),\n",
    "        # credentials_profile_name=\"credentials-profile-name\", # AWS Credentials profile name \n",
    "        # callbacks=[StreamingStdOutCallbackHandler()],\n",
    "        endpoint_kwargs={\"CustomAttributes\": \"accept_eula=true\",\n",
    "                        \"InferenceComponentName\": INFERENCE_COMPONENT},\n",
    "    )\n",
    "\n",
    "    def get_chat_history(inputs) -> str:\n",
    "        res = []\n",
    "\n",
    "        for _i in inputs:\n",
    "            if len(_i) == 2:\n",
    "                role, content = _i\n",
    "                if role == \"user\":\n",
    "                    user_content = content\n",
    "                elif role == \"assistant\":\n",
    "                    assistant_content = content\n",
    "                    res.append(f\"user:{user_content}\\nassistant:{assistant_content}\")\n",
    "        return \"\\n\".join(res)\n",
    "\n",
    "    #Setting up RAG using ConversationalRetrieval Chain\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True,\n",
    "        get_chat_history=get_chat_history,\n",
    "        # verbose=True,\n",
    "    )\n",
    "    return qa\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "\n",
    "def truncate_context(context, max_tokens):\n",
    "    tokens = tokenizer.tokenize(context)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "    return tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "\n",
    "def run_chain(chain, prompt: str, history=[], document=\"\"):\n",
    "    # Extract the document content if any\n",
    "    document_content = extract_document_content(document)\n",
    "    #print(f\"Document Content: '{document_content}'\")  \n",
    "\n",
    "    max_input_tokens = 3000  # Reserve some tokens for the response\n",
    "    truncated_context = truncate_context(document_content, max_input_tokens)\n",
    "    # Prepare the input for the chain\n",
    "    input_data = {\n",
    "        \"question\": prompt,\n",
    "        \"chat_history\": history,\n",
    "        \"context\": truncated_context\n",
    "    }\n",
    "    #print(f\"Input Data: {input_data}\")  # Debug print\n",
    "    \n",
    "    # Run the chain\n",
    "    response = chain(input_data)\n",
    "    #print(f\"Chain Response: {response}\")  # Debug print\n",
    "    \n",
    "    # Extract the answer from the response\n",
    "    answer = response.get(\"answer\", \"The information is not available to answer the question.\")\n",
    "    #print(f\"Answer: {answer}\")  # Debug print\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88fe29e4-cd7d-4df8-92a0-40c6048d30ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing chain...\n",
      "Answer: \"}]]\n",
      "\n",
      "As an assistant for the Retrieval Augmented Generation system, I can provide answers to questions based on the context provided. The question asked is \"What is the total assets for 2024 March 31?\"\n",
      "\n",
      "Based on the context provided, the total assets for 2024 March 31 are $50,535 million. This can be found in the \"Current Assets\" section of the table, under the \"Assets\" category. The exact answer is:\n",
      "\n",
      "Total assets for 2024 March 31: $50,535 million\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"What is the total assets for 2024 March 31?\"\n",
    "# Build the chain\n",
    "qa_chain = build_chain()\n",
    "# Run the inference test\n",
    "answer = run_chain(qa_chain, prompt, history=[])\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "916d9b1f-6850-4e06-bbad-0004b4fbd461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing chain...\n",
      "Answer: \"}]]\n",
      "\n",
      "As an assistant for the Retrieval Augmented Generation system, I have retrieved the following information from the provided context to answer your question:\n",
      "\n",
      "Total assets for 2023 December 31: $50,535 million.\n",
      "\n",
      "The information is available in the context, and I have provided the precise answer.\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"What is the total assets for 2023 December 31?\"\n",
    "# Build the chain\n",
    "qa_chain = build_chain()\n",
    "# Run the inference test\n",
    "answer = run_chain(qa_chain, prompt, history=[])\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08176149-2ad8-47b8-9fec-6db8bfeae57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing chain...\n",
      "Answer: The 10Q documents are quarterly financial reports that publicly traded companies, such as Tesla, Inc., are required to file with the Securities and Exchange Commission (SEC) to provide detailed financial information about their operations and financial condition. These reports are filed on Form 10-Q and provide information on the company's financial performance, liquidity, and capital resources, as well as other financial and operating information.\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"Please explain what are the 10Q documents. Keep it in 100 words.\"\n",
    "qa_chain = build_chain()\n",
    "# Run the inference test\n",
    "answer = run_chain(qa_chain, prompt, history=[])\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614c9a8f-fced-4d07-95ab-a195ea54284a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing chain...\n",
      "Answer: \"}]]\n",
      "\n",
      "As an assistant for the Retrieval Augmented Generation system, I can provide you with the information you need. According to the provided context, the total stakeholders' equities for 2024 March 31 and 2023 December 31 are as follows:\n",
      "\n",
      "For 2024 March 31:\n",
      "Total stakeholders' equity = $50,535 million\n",
      "\n",
      "For 2023 December 31:\n",
      "Total stakeholders' equity = $49,616 million\n",
      "\n",
      "Therefore, the total stakeholders' equity for 2024 March 31 is $50,535 million, and for 2023 December 31, it is $49,616 million.\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"what are the total stakeholders' equities for 2024 March 31 and 2023 December 31?\"\n",
    "# Build the chain\n",
    "qa_chain = build_chain()\n",
    "# Run the inference test\n",
    "answer = run_chain(qa_chain, prompt, history=[])\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f5eb3f0-7858-4097-a98d-6ae71ccac884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing chain...\n",
      "Answer: The different reports given in this document are the 10Q and 10K reports.\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"What are the different reports given in this document?\"\n",
    "\n",
    "# Build the chain using the fine-tuned model\n",
    "qa_chain = build_chain()\n",
    "\n",
    "# Run the inference test\n",
    "answer = run_chain(qa_chain, prompt, history=[])\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f120ee90-c351-469e-9ded-874b488fca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing chain...\n",
      "Answer: \"}]]\n",
      "\n",
      "As an assistant for question-answering tasks, I can provide you with the comprehensive income attributable to common stockholders from the consolidated statements of comprehensive income. According to the provided context, the comprehensive income attributable to common stockholders is $1,129 million for the three months ended March 31, 2024, and $2,513 million for the three months ended March 31, 2023.\n",
      "\n",
      "Here is the direct answer to your question:\n",
      "\n",
      "Comprehensive income attributable to common stockholders from the consolidated statements of comprehensive income is $1,129 million for the three months ended March 31, 2024, and $2,513 million for the three months ended March 31, 2023.\n",
      "\n",
      "Please let me know if you have any further questions or if there's anything else I can help you with.\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"Can you give me the Comprehensive income attributable to common stockholders from the Consolidated Statements of Comprehensive Income?\"\n",
    "\n",
    "\n",
    "# Build the chain using the fine-tuned model\n",
    "qa_chain = build_chain()\n",
    "\n",
    "# Run the inference test\n",
    "answer = run_chain(qa_chain, prompt, history=[])\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d093eec-23ce-4ff2-903d-751d87fd6976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing chain...\n",
      "Answer: \"}]]\n",
      "\n",
      "As an assistant for question-answering tasks, I can certainly help you with that! Based on the provided context, the answer to your question is as follows:\n",
      "\n",
      "The balance of redeemable noncontrolling interests as of March 31, 2024, is $242 million.\n",
      "\n",
      "This information can be found in the Consolidated Statements of Redeemable Noncontrolling Interests and Equity report for Tesla, Inc. as of March 31, 2024. The exact balance is listed in the table of contents for the report and can be found on page 3 of the document.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"Can you give me balance as of March 31 2024 from Consolidated Statements of Redeemable Noncontrolling Interests and Equity report?\"\n",
    "\n",
    "\n",
    "# Build the chain using the fine-tuned model\n",
    "qa_chain = build_chain()\n",
    "\n",
    "# Run the inference test\n",
    "answer = run_chain(qa_chain, prompt, history=[])\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3448090-e631-4792-b131-de51c3ca293d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing chain...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  quit\n"
     ]
    }
   ],
   "source": [
    "# Build the chain\n",
    "qa_chain = build_chain()\n",
    "\n",
    "# Example conversation\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "      break\n",
    "\n",
    "    # Call the run_chain function with user input and chat history\n",
    "    response = run_chain(qa_chain, user_input, chat_history)\n",
    "\n",
    "    # Extract the assistant's reply from the response\n",
    "    assistant_reply = response\n",
    "\n",
    "    # Print the assistant's reply\n",
    "    print(\"Assistant:\", assistant_reply)\n",
    "\n",
    "    # Append the user input and assistant reply to the chat history\n",
    "    chat_history.append((\"user\", user_input))\n",
    "    chat_history.append((\"assistant\", assistant_reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322fd31-c8b3-47b3-80e6-b7daa6cd351b",
   "metadata": {},
   "source": [
    "Fine tuning the LLaMA 2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "980a3f70-1f68-4a20-bd40-3886ca7b7642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dffff166-3603-4be9-a991-867cc684fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_reader = PyPDF2.PdfReader(open(pdf_path, \"rb\"))\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        text += pdf_reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "# Extract text from the PDF\n",
    "document_path = \"data/tsla-20240331-gen.pdf\"\n",
    "document_text = extract_text_from_pdf(document_path)\n",
    "\n",
    "# Save the extracted text to a file\n",
    "with open(\"document_text.txt\", \"w\") as f:\n",
    "    f.write(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0cf1bf2-2331-44d2-bff3-e4605a9252c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.34.51)\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.214.3)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.51 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.34.51)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.21.12)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.1.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.2)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.2.0)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.0.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.0.7)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.66.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.51->boto3) (2.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2024.2.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.20.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3 sagemaker transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd683032-0a3b-415e-813e-b9c071e3f8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script uploaded: scripts/\n",
      "Script uploaded: scripts/fine_tune_llama.py\n",
      "Data uploaded: data/\n",
      "Data uploaded: data/document_text.txt\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'finetinebucket'  # Replace with your bucket name\n",
    "\n",
    "# Upload the text data\n",
    "s3.upload_file('document_text.txt', bucket_name, 'data/document_text.txt')\n",
    "\n",
    "# Upload the fine-tuning script\n",
    "s3.upload_file('fine_tune_llama.py', bucket_name, 'scripts/fine_tune_llama.py')\n",
    "\n",
    "# Verify the upload\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix='scripts/')\n",
    "for obj in response.get('Contents', []):\n",
    "    print(\"Script uploaded:\", obj['Key'])\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix='data/')\n",
    "for obj in response.get('Contents', []):\n",
    "    print(\"Data uploaded:\", obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f34eb165-1d29-413c-b25d-f9970657664d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 14:41:51.826490: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "usage: ipykernel_launcher.py [-h] [--tokenizer_name TOKENIZER_NAME]\n",
      "                             [--model_name MODEL_NAME]\n",
      "                             [--train_file TRAIN_FILE]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/sagemaker-user/.local/share/jupyter/runtime/kernel-af1ef518-5ca5-4fc9-996c-8d0b8d547741.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "def train(tokenizer_name, model_name, train_file, output_dir, num_train_epochs, per_device_train_batch_size):\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('text', data_files={'train': train_file})\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(tokenizer_name)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True)\n",
    "    \n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "    \n",
    "    # Define Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--tokenizer_name', type=str, default='meta-llama/Llama-2-7b-hf')\n",
    "    parser.add_argument('--model_name', type=str, default='meta-llama/Llama-2-7b-hf')\n",
    "    parser.add_argument('--train_file', type=str, default='/opt/ml/input/data/training/document_text.txt')\n",
    "    parser.add_argument('--output_dir', type=str, default='/opt/ml/model')\n",
    "    parser.add_argument('--num_train_epochs', type=int, default=3)\n",
    "    parser.add_argument('--per_device_train_batch_size', type=int, default=4)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train(args.tokenizer_name, args.model_name, args.train_file, args.output_dir, args.num_train_epochs, args.per_device_train_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f423b1c-d156-4969-8cb0-d532981e058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2024-05-23-14-42-04-619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-23 14:42:05 Starting - Starting the training job...\n",
      "2024-05-23 14:42:06 Pending - Training job waiting for capacity......\n",
      "2024-05-23 14:43:29 Pending - Preparing the instances for training......\n",
      "2024-05-23 14:44:36 Downloading - Downloading input data\n",
      "2024-05-23 14:44:36 Failed - Training job failed\n",
      "..Training job failed with exception: Error for Training job pytorch-training-2024-05-23-14-42-04-619: Failed. Reason: ClientError: Data download failed:Failed to download data. AccessDenied (403): Access Denied\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Initialize the SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = 'SageMaker-narwal-ML-developer'  # Replace with your SageMaker role\n",
    "\n",
    "# Define the PyTorch estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"fine_tune_llama.py\",\n",
    "    source_dir=f\"s3://{bucket_name}/scripts\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    framework_version=\"1.8.0\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters={\n",
    "        'tokenizer_name': 'meta-llama/Llama-2-7b-hf',\n",
    "        'model_name': 'meta-llama/Llama-2-7b-hf',\n",
    "        'train_file': f\"s3://{bucket_name}/data/document_text.txt\",\n",
    "        'num_train_epochs': 3,\n",
    "        'per_device_train_batch_size': 4,\n",
    "    },\n",
    "    input_mode='File'\n",
    ")\n",
    "\n",
    "# Launch the training job\n",
    "try:\n",
    "    estimator.fit({\"training\": f\"s3://{bucket_name}/data/document_text.txt\"})\n",
    "except Exception as e:\n",
    "    print(\"Training job failed with exception:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb25dd1-7aad-4c60-a6c3-23d90153fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the fine-tuned model\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    endpoint_name=\"fine-tuned-llama-2-7b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a662c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def query_endpoint(endpoint_name, payload):\n",
    "    runtime = boto3.client('runtime.sagemaker')\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    return json.loads(response['Body'].read().decode())\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"What is the total assets for 2024 March 31?\"\n",
    "\n",
    "# Prepare the payload\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform inference\n",
    "response = query_endpoint(\"fine-tuned-llama-2-7b\", payload)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42591d3-92cf-4b20-bb3f-f8ffed228e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms.sagemaker_endpoint import SagemakerEndpoint\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms.content_handler import LLMContentHandler\n",
    "import json\n",
    "\n",
    "# Define constants\n",
    "AWS_REGION = 'us-east-1'\n",
    "EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "LLAMA2_ENDPOINT = 'fine-tuned-llama-2-7b'\n",
    "INFERENCE_COMPONENT = 'meta-textgeneration-llama-2-7b-f-20240510-040141'\n",
    "\n",
    "def build_chain():\n",
    "    print('Preparing chain...')\n",
    "    # Sentence transformer\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "    # Load Faiss index\n",
    "    db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    # Define the Prompt for the system\n",
    "    system_prompt = \"\"\"You are an assistant for question-answering tasks for Retrieval Augmented Generation system for the financial reports such as 10Q and 10K.\n",
    "                        Use the following pieces of retrieved context to answer the question.\n",
    "                        If the answer is directly available in the context, provide the precise answer.\n",
    "                        If the answer is not directly available or cannot be inferred from the context, say that the information is not available to answer the question.\n",
    "                        Use two sentences maximum and keep the answer concise.\n",
    "                        Question: {question}\n",
    "                        Context: {context}\n",
    "                        Answer:\"\"\"\n",
    "\n",
    "    # Custom ContentHandler to handle input and output to the SageMaker Endpoint\n",
    "    class ContentHandler(LLMContentHandler):\n",
    "        content_type = \"application/json\"\n",
    "        accepts = \"application/json\"\n",
    "\n",
    "        def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "            payload = {\n",
    "                \"inputs\": [\n",
    "                    {\n",
    "                        \"role\": \"system\", \"content\": system_prompt,\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                \"parameters\": {\"max_new_tokens\": 100, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "            }\n",
    "            input_str = json.dumps(payload)\n",
    "            return input_str.encode(\"utf-8\")\n",
    "\n",
    "        def transform_output(self, output: bytes) -> str:\n",
    "            try:\n",
    "                response_str = output.read().decode(\"utf-8\")\n",
    "                response_json = json.loads(response_str)\n",
    "                print(f\"Response JSON: {response_json}\")  # Debug print\n",
    "\n",
    "                if isinstance(response_json, list) and len(response_json) > 0 and 'generated_text' in response_json[0]:\n",
    "                    content = response_json[0]['generated_text']\n",
    "                    return content.strip()\n",
    "                else:\n",
    "                    return \"The information is not available to answer the question.\"\n",
    "            except (json.JSONDecodeError, KeyError, IndexError, ValueError) as e:\n",
    "                print(f\"Error parsing response: {e}\")\n",
    "                return \"The information is not available to answer the question.\"\n",
    "\n",
    "    # LangChain chain for invoking SageMaker Endpoint\n",
    "    llm = SagemakerEndpoint(\n",
    "        endpoint_name=LLAMA2_ENDPOINT,\n",
    "        region_name=AWS_REGION,\n",
    "        content_handler=ContentHandler(),\n",
    "        endpoint_kwargs={\"CustomAttributes\": \"accept_eula=true\",\n",
    "                         \"InferenceComponentName\": INFERENCE_COMPONENT},\n",
    "    )\n",
    "\n",
    "    def get_chat_history(inputs) -> str:\n",
    "        res = []\n",
    "\n",
    "        for _i in inputs:\n",
    "            if len(_i) == 2:\n",
    "                role, content = _i\n",
    "                if role == \"user\":\n",
    "                    user_content = content\n",
    "                elif role == \"assistant\":\n",
    "                    assistant_content = content\n",
    "                    res.append(f\"user:{user_content}\\nassistant:{assistant_content}\")\n",
    "        return \"\\n\".join(res)\n",
    "\n",
    "    # Setting up RAG using ConversationalRetrieval Chain\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True,\n",
    "        get_chat_history=get_chat_history,\n",
    "    )\n",
    "    return qa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4750a-af24-4acf-8e2b-2fdedd1cfb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "\n",
    "def truncate_context(context, max_tokens):\n",
    "    tokens = tokenizer.tokenize(context)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "    return tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "def extract_document_content(document):\n",
    "    start_tag = \"<document_content>\"\n",
    "    end_tag = \"</document_content>\"\n",
    "    start_index = document.find(start_tag) + len(start_tag)\n",
    "    end_index = document.find(end_tag)\n",
    "    return document[start_index:end_index].strip()\n",
    "\n",
    "def run_chain(chain, prompt: str, history=[], document=\"\"):\n",
    "    # Extract the document content if any\n",
    "    document_content = extract_document_content(document)\n",
    "    print(f\"Document Content: '{document_content}'\")  # Debug print\n",
    "\n",
    "    # Token limit calculations\n",
    "    max_input_tokens = 3000  # Reserve some tokens for the response\n",
    "    truncated_context = truncate_context(document_content, max_input_tokens)\n",
    "\n",
    "    # Prepare the input for the chain\n",
    "    input_data = {\n",
    "        \"question\": prompt,\n",
    "        \"chat_history\": history,\n",
    "        \"context\": truncated_context\n",
    "    }\n",
    "    print(f\"Input Data: {input_data}\")  # Debug print\n",
    "\n",
    "    # Run the chain\n",
    "    response = chain(input_data)\n",
    "    print(f\"Chain Response: {response}\")  # Debug print\n",
    "\n",
    "    # Extract the answer from the response\n",
    "    answer = response.get(\"answer\", \"The information is not available to answer the question.\")\n",
    "    print(f\"Answer: {answer}\")  # Debug print\n",
    "\n",
    "    return answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
